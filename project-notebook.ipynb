{
 "cells": [
  {
   "source": [
    "# MIMIC NOTEEVENTS PRE-PROCESSING"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnoses = pd.read_csv('mimicdata/mimic3/DIAGNOSES_ICD.csv')\n",
    "d_icd_diagnoses = pd.read_csv('mimicdata/D_ICD_DIAGNOSES.csv')\n",
    "d_icd_procedures = pd.read_csv('mimicdata/D_ICD_PROCEDURES.csv')\n",
    "noteevents = pd.read_csv('mimicdata/mimic3/NOTEEVENTS.csv', nrows=10)\n",
    "print(noteevents.iloc[0])\n",
    "print(noteevents.iloc[0]['TEXT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.path)\n",
    "import datasets\n",
    "import log_reg\n",
    "from dataproc import extract_wvs\n",
    "from dataproc import get_discharge_summaries\n",
    "from dataproc import concat_and_split\n",
    "from dataproc import build_vocab\n",
    "from dataproc import vocab_index_descriptions\n",
    "from dataproc import word_embeddings\n",
    "from constants import MIMIC_3_DIR, DATA_DIR\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import csv\n",
    "import math\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 'full' #use all available labels in the dataset for prediction\n",
    "print(MIMIC_3_DIR)\n",
    "notes_file = '%s/NOTEEVENTS.csv' % MIMIC_3_DIR # raw note events downloaded from MIMIC-III\n",
    "vocab_size = 'full' #don't limit the vocab size to a specific number\n",
    "vocab_min = 3 #discard tokens appearing in fewer than this many documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(notes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfproc = pd.read_csv('%s/PROCEDURES_ICD.csv' % MIMIC_3_DIR)\n",
    "dfdiag = pd.read_csv('%s/DIAGNOSES_ICD.csv' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdiag['absolute_code'] = dfdiag.apply(lambda row: str(datasets.reformat(str(row[4]), True)), axis=1)\n",
    "dfproc['absolute_code'] = dfproc.apply(lambda row: str(datasets.reformat(str(row[4]), False)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcodes = pd.concat([dfdiag, dfproc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcodes.to_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, index=False,\n",
    "               columns=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'absolute_code'],\n",
    "               header=['ROW_ID', 'SUBJECT_ID', 'HADM_ID', 'SEQ_NUM', 'ICD9_CODE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the full dataset (not just discharge summaries)\n",
    "df = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR, dtype={\"ICD9_CODE\": str})\n",
    "len(df['ICD9_CODE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This reads all notes, selects only the discharge summaries, and tokenizes them, returning the output filename\n",
    "disch_full_file = get_discharge_summaries.write_discharge_summaries(out_file=\"%s/disch_full.csv\" % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('%s/disch_full.csv' % MIMIC_3_DIR)\n",
    "#How many admissions?\n",
    "len(df['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in df.itertuples():\n",
    "    for w in row[4].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num types\", len(types))\n",
    "print(\"Num tokens\", str(num_tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's sort by SUBJECT_ID and HADM_ID to make a correspondence with the MIMIC-3 label file\n",
    "df = df.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort the label file by the same\n",
    "dfl = pd.read_csv('%s/ALL_CODES.csv' % MIMIC_3_DIR)\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['HADM_ID'].unique()), len(dfl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's filter out these HADM_ID's\n",
    "hadm_ids = set(df['HADM_ID'])\n",
    "with open('%s/ALL_CODES.csv' % MIMIC_3_DIR, 'r') as lf:\n",
    "    with open('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, 'w') as of:\n",
    "        w = csv.writer(of)\n",
    "        w.writerow(['SUBJECT_ID', 'HADM_ID', 'ICD9_CODE', 'ADMITTIME', 'DISCHTIME'])\n",
    "        r = csv.reader(lf)\n",
    "        #header\n",
    "        next(r)\n",
    "        for i,row in enumerate(r):\n",
    "            hadm_id = int(row[2])\n",
    "            #print(hadm_id)\n",
    "            #break\n",
    "            if hadm_id in hadm_ids:\n",
    "                w.writerow(row[1:3] + [row[-1], '', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfl = pd.read_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we still need to sort it by HADM_ID\n",
    "dfl = dfl.sort_values(['SUBJECT_ID', 'HADM_ID'])\n",
    "dfl.to_csv('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's append each instance with all of its codes\n",
    "#this is pretty non-trivial so let's use this script I wrote, which requires the notes to be written to file\n",
    "sorted_file = '%s/disch_full.csv' % MIMIC_3_DIR\n",
    "df.to_csv(sorted_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled = concat_and_split.concat_data('%s/ALL_CODES_filtered.csv' % MIMIC_3_DIR, sorted_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnl = pd.read_csv(labeled)\n",
    "#Tokens and types\n",
    "types = set()\n",
    "num_tok = 0\n",
    "for row in dfnl.itertuples():\n",
    "    for w in row[3].split():\n",
    "        types.add(w)\n",
    "        num_tok += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"num types\", len(types), \"num tokens\", num_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfnl['HADM_ID'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = '%s/notes_labeled.csv' % MIMIC_3_DIR\n",
    "base_name = \"%s/disch\" % MIMIC_3_DIR #for output\n",
    "tr, dv, te = concat_and_split.split_data(fname, base_name=base_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_min = 3\n",
    "vname = '%s/vocab.csv' % MIMIC_3_DIR\n",
    "build_vocab.build_vocab(vocab_min, tr, vname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/disch_%s_split.csv' % (MIMIC_3_DIR, splt)\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_full.csv' % (MIMIC_3_DIR, splt), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_file = word_embeddings.word_embeddings('full', '%s/disch_full.csv' % MIMIC_3_DIR, 100, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_wvs.gensim_to_embeddings('%s/processed_full.w2v' % MIMIC_3_DIR, '%s/vocab.csv' % MIMIC_3_DIR, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MIMIC_3_DIR)\n",
    "vocab_index_descriptions.vocab_index_descriptions('%s/vocab.csv' % MIMIC_3_DIR,\n",
    "                                                  '%s/description_vectors.vocab' % MIMIC_3_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = 50\n",
    "#first calculate the top k\n",
    "counts = Counter()\n",
    "dfnl = pd.read_csv('%s/notes_labeled.csv' % MIMIC_3_DIR)\n",
    "for row in dfnl.itertuples():\n",
    "    for label in str(row[4]).split(';'):\n",
    "        counts[label] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "codes_50 = sorted(counts.items(), key=operator.itemgetter(1), reverse=True)\n",
    "codes_50 = [code[0] for code in codes_50[:Y]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('%s/TOP_%s_CODES.csv' % (MIMIC_3_DIR, str(Y)), 'w') as of:\n",
    "    w = csv.writer(of)\n",
    "    for code in codes_50:\n",
    "        w.writerow([code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    print(splt)\n",
    "    hadm_ids = set()\n",
    "    with open('%s/%s_50_hadm_ids.csv' % (MIMIC_3_DIR, splt), 'r') as f:\n",
    "        for line in f:\n",
    "            hadm_ids.add(line.rstrip())\n",
    "    with open('%s/notes_labeled.csv' % MIMIC_3_DIR, 'r') as f:\n",
    "        with open('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), 'w') as of:\n",
    "            r = csv.reader(f)\n",
    "            w = csv.writer(of)\n",
    "            #header\n",
    "            w.writerow(next(r))\n",
    "            i = 0\n",
    "            for row in r:\n",
    "                hadm_id = row[1]\n",
    "                if hadm_id not in hadm_ids:\n",
    "                    continue\n",
    "                codes = set(str(row[3]).split(';'))\n",
    "                filtered_codes = codes.intersection(set(codes_50))\n",
    "                if len(filtered_codes) > 0:\n",
    "                    w.writerow(row[:3] + [';'.join(filtered_codes)])\n",
    "                    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for splt in ['train', 'dev', 'test']:\n",
    "    filename = '%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y))\n",
    "    df = pd.read_csv(filename)\n",
    "    df['length'] = df.apply(lambda row: len(str(row['TEXT']).split()), axis=1)\n",
    "    df = df.sort_values(['length'])\n",
    "    df.to_csv('%s/%s_%s.csv' % (MIMIC_3_DIR, splt, str(Y)), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATIC FEATURE GENERATION\n",
    "## DEMOGRAPHIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Static Features\n",
    "## \n",
    "## -> length of stay\n",
    "## -> admission type\n",
    "## -> admission dx\n",
    "## -> admission location\n",
    "## -> discharge location\n",
    "## -> ethnicity\n",
    "## -> gender\n",
    "## -> age (at admission)\n",
    "## -> insurance\n",
    "## -> marital status\n",
    "\n",
    "\n",
    "path_to_files = '<<CS598-DLHC/MIMIC-III/>>'\n",
    "\n",
    "patients = pd.read_csv(path_to_files + 'PATIENTS.csv', parse_dates=['DOB'])\n",
    "admissions = pd.read_csv(path_to_files + 'ADMISSIONS.csv', parse_dates=['ADMITTIME', 'DISCHTIME'])\n",
    "\n",
    "assert len(patients['SUBJECT_ID'].unique()) == len(patients['SUBJECT_ID'])\n",
    "patients = patients.set_index(\"SUBJECT_ID\")\n",
    "\n",
    "static_df = admissions.join(patients, on='SUBJECT_ID', how='left', rsuffix='pat')\n",
    "assert len(admissions) == len(static_df)\n",
    "\n",
    "def bin_age(age):\n",
    "    if age < 25:\n",
    "        return '18-25'\n",
    "    elif age < 45:\n",
    "        return '25-45'\n",
    "    elif age < 65:\n",
    "        return '45-65'\n",
    "    elif age < 89:\n",
    "        return '65-89'\n",
    "    else:\n",
    "        return '89+'\n",
    "    \n",
    "def bin_los(los):\n",
    "    if los < 2:\n",
    "        return '1-2'\n",
    "    elif los < 4:\n",
    "        return '3-4'\n",
    "    elif los < 7:\n",
    "        return '5-7'\n",
    "    elif los < 10:\n",
    "        return '8-10'\n",
    "    elif los < 15:\n",
    "        return '10-15'\n",
    "    else:\n",
    "        return '15+'\n",
    "\n",
    "## Length of Stay    \n",
    "static_df['LENGTH_OF_STAY'] = (static_df['DISCHTIME'] - static_df['ADMITTIME']) / np.timedelta64(1, 'D')\n",
    "static_df['LENGTH_OF_STAY'] = static_df['LENGTH_OF_STAY'].apply(bin_los)\n",
    "\n",
    "## Age\n",
    "static_df['AGE'] = static_df['ADMITTIME'].subtract(static_df['DOB']).dt.days / 365.242\n",
    "static_df['AGE'] = static_df['AGE'].apply(bin_age)\n",
    "\n",
    "## Output DF\n",
    "static_df = static_df.filter(items=['HADM_ID',\n",
    "                                    'SUBJECT_ID',\n",
    "                                    'LENGTH_OF_STAY',\n",
    "                                    'ADMISSION_TYPE',\n",
    "                                    'DIAGNOSIS',\n",
    "                                    'ADMISSION_LOCATION',\n",
    "                                    'DISCHARGE_LOCATION',\n",
    "                                    'ETHNICITY',\n",
    "                                    'GENDER',\n",
    "                                    'AGE',\n",
    "                                    'INSURANCE',\n",
    "                                    'MARITAL_STATUS'])\n",
    "\n",
    "static_df.to_csv('output_df.csv')\n",
    "for col in static_df.columns:\n",
    "    print(len(static_df[col].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_static_features(input_df, out_path = ''):\n",
    "    feature_dict = dict()\n",
    "    feature_val_dict = dict()\n",
    "\n",
    "\n",
    "    columns = input_df.columns\n",
    "    column_to_index = dict()\n",
    "    for i, col in enumerate(columns):\n",
    "        column_to_index[col] = i\n",
    "\n",
    "    for i, row in input_df.iterrows():\n",
    "        hadm_id = row[0]\n",
    "        feature_dict[hadm_id] = []\n",
    "        for col in columns:\n",
    "            if col not in ['DIAGNOSIS','HADM_ID','SUBJECT_ID']:\n",
    "                val = row[column_to_index[col]]\n",
    "                if val not in feature_val_dict:\n",
    "                    feature_val_dict[val] = len(feature_val_dict)\n",
    "                feature_dict[hadm_id].append(feature_val_dict[val])\n",
    "    json.dump(feature_dict, open(os.path.join(out_path, 'static_feature_dict.json'), 'w'))\n",
    "    json.dump(feature_val_dict, open(os.path.join(out_path, 'static_feature_index_dict.json'), 'w'))\n",
    "\n",
    "generate_static_features(static_df)"
   ]
  },
  {
   "source": [
    "## MEDICATION DATA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meds = pd.read_csv(path_to_files + 'PRESCRIPTIONS.csv')\n",
    "\n",
    "\n",
    "hadm_id = list(df['HADM_ID'].unique())\n",
    "meds = meds[meds['HADM_ID'].isin(hadm_id)]\n",
    "meds[meds['HADM_ID'] == 134157]\n",
    "print(len(hadm_id))\n",
    "\n",
    "meds_trimmed = meds.filter(items=['HADM_ID',\n",
    "                                  'NDC'])\n",
    "\n",
    "def generate_med_features(hadm_ids, med_df, out_path = ''):\n",
    "    med_feature_dict = dict()\n",
    "    meds_dict = dict()\n",
    "    max_meds = 0\n",
    "    \n",
    "    for hadm_id in hadm_ids:\n",
    "        hadm_id = str(int(hadm_id))\n",
    "        med_feature_dict[hadm_id] = []\n",
    "        \n",
    "    for i, row in tqdm(med_df.iterrows()):\n",
    "        hadm_id = str(int(row[0]))\n",
    "        ndc = str(row[1])\n",
    "        if ndc not in meds_dict:\n",
    "            meds_dict[ndc] = len(meds_dict) + 1\n",
    "        med_feature_dict[hadm_id].append(meds_dict[ndc])\n",
    "        max_meds = max(max_meds, len(med_feature_dict[hadm_id]))\n",
    "               \n",
    "    json.dump(med_feature_dict, open(os.path.join(out_path, 'meds_feature_dict.json'), 'w'))\n",
    "    json.dump(meds_dict, open(os.path.join(out_path, 'meds_feature_index_dict.json'), 'w'))\n",
    "\n",
    "generate_med_features(hadm_id, meds_trimmed)"
   ]
  },
  {
   "source": [
    "# MODEL TRAINING\n",
    "## TRAIN NEW ALPACA MODEL - 50 Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python learn/training.py mimicdata/mimic3/train_50.csv mimicdata/mimic3/vocab.csv 50 alpaca 200 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --criterion prec_at_5 --lr 0.0001 --embed-file mimicdata/mimic3/processed_full.embed --gpu"
   ]
  },
  {
   "source": [
    "## TRAIN NEW ALPACA MODEL - FULL Labels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python learn/training.py mimicdata/mimic3/train_full.csv mimicdata/mimic3/vocab.csv full alpaca 150 --filter-size 10 --num-filter-maps 50 --dropout 0.2 --patience 10 --lr 0.0001 --criterion prec_at_8 --embed-file mimicdata/mimic3/processed_full.embed --gpu ## --meds=1 --med-embed-size=100 --med-pool-size=5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}